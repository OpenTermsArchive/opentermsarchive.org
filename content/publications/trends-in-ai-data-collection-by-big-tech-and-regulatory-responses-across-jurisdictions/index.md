---
title: Trends in AI data collection by Big Tech and regulatory responses across jurisdictions
html_description:
author: Saumyaa Naidu
regulations: ["AI Act", "DSA", "DMA"]
topic: ["Privacy"]
date: 2026-01-16
---

In 2025, several big tech companies updated their AI-related policies. Through tracking and analysing updates in these policies throughout 2025 using Open Terms Archive, shifts were detected and a broader trend emerged: large tech companies intensifying data collection for AI training, including AI interaction data. It also revealed a comparative view of the extent of data collection, sharing, and usage in the platforms across jurisdictions.

For example, Google [announced](https://blog.google/products/gemini/deep-research-workspace-app-integration/) in November that its AI product, Gemini Deep Research, can access user information from their Gmail, Drive, and Chat accounts, and rolled out this change globally.  
Fragmentation appears in the case of LinkedIn, which [began](https://opentermsarchive.org/en/memos/linkedin-shares-data-with-microsoft-and-its-subsidiaries-to-train-ai-models/) collecting user profile data and public posts to train its generative AI systems and enhance its advertising in regions such as the European Union (EU), United Kingdom (UK), Switzerland, Canada, and Hong Kong. This use of data for AI training was [already in effect](https://www.business-standard.com/technology/tech-news/linkedin-will-use-your-profile-data-to-train-its-ai-model-how-to-opt-out-125092500744_1.html) in all other jurisdictions including the United States (US). The expansion was made while offering a manual opt-out to the new jurisdictions.  
Further difference is illustrated by Meta, which simply excluded the EU, the UK and South Korea from its December policy [update](https://opentermsarchive.org/en/memos/instagram-begins-use-of-ai-interactions-to-personalise-experiences-and-ads/). This update uses data from interactions with AI tools to personalise ads and content on Instagram and Facebook. In the US and other parts of the world, this change has been applied with no opt-out available to users.

These updates can bring severe privacy risks for people using the platforms. User trust in AI chat bots [leads](https://hai.stanford.edu/news/privacy-ai-era-how-do-we-protect-our-personal-information) to them sharing personal or sensitive data which AI systems may later mishandle, leak, or use for unintended purposes. A common feature in these updates is the cross-sharing of data within the larger entity. LinkedIn, for instance, will [share](https://www.bitdefender.com/en-us/blog/hotforsecurity/linkedin-gives-you-until-monday-to-stop-ai-from-training-on-your-profile) data with other Microsoft-related business entities, for the purposes of serving "more personalised and relevant ads.” Meta has also integrated data across all its products in order to personalise features, content, and ads. The information will also be shared with other Meta Companies.  

## Concerns raised by civil society

In response to the change announced by Meta in October, a coalition of 36 US-based[^1] civil society organisations wrote a [letter](https://epic.org/documents/letter-calling-for-ftc-oversight-and-suspension-of-metas-ai-chatbot-advertising-practice/%20) calling for Federal Trade Commission (FTC) oversight and suspension of Meta’s AI data use for advertising. The letter [asks](https://epic.org/press-release-advocates-urge-ftc-to-halt-metas-plan-to-use-ai-chatbot-data-for-ads/%20) the FTC to “enforce Meta’s existing consent decrees and require disclosure of risk assessments; treat the practice as an unfair and deceptive act under Section 5 of the FTC Act; suspend Meta’s chatbot advertising program pending Commission review; and finalise the long-pending modifications to the 2020 order to strengthen privacy protections, including a proposed prohibition to monetise minors’ data.” The coalition highlighted that Meta’s initiative is part of a larger strategy to expand surveillance-driven marketing and warned that without the FTC’s intervention at this time, such invasive AI data practices will become the norm, leaving consumers unprotected. The letter also emphasises how such surveillance-driven and behavioral marketing is shaping how people spend their money, time, and attention, and can lead to exploitative targeting. 

## Regulatory background and responses in the US and EU

In a regulatory response to these shifts by big tech organisations, the US has positioned itself to be “pro-innovation” with limited AI regulation, while the EU has taken a rights-oriented stance with more detailed and prescriptive laws. Presently, the US relies on existing regulatory bodies and guidelines and initiatives from various agencies to address AI issues, the FTC being one of these agencies. While the FTC is legally established as an independent agency, its status is being challenged in an ongoing legal and political [debate](https://iapp.org/news/a/us-supreme-court-tackles-legal-questions-prior-precedent-in-firing-of-ftc-s-slaughter). The supreme court ruling on the matter is due in the next few months and could have far-reaching consequences for other independent agencies in the country. The US also has [varying state laws](https://www.congress.gov/crs-product/R48555) for AI, focusing on transparency, consumer protection, and high-risk systems. 

More recently, President Trump signed an [executive order](https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/) on December 11, 2025, to curb state regulation of AI, declaring it a US federal strategic priority and centralising federal oversight. The order [directs](https://www.techpolicy.press/a-critical-look-at-trumps-ai-executive-order/) key agencies to coordinate AI policy, and creates an AI litigation task force to challenge state laws and identify state measures seen as hindering US AI leadership. President Trump has [reiterated](https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/) in the order his objective of preserving US leadership and asserting its “dominance” in AI. Instead of the previous administration’s strong emphasis on mitigating AI-related risks, his administration pursues a strategy that prioritises promoting innovation and responding to China’s expanding technological influence. The US is also [looking](https://www.techpolicy.press/the-good-bad-and-really-weird-ai-provisions-in-the-annual-us-defense-policy-bill/%20) to pass the National Defense Authorization Act (NDAA), which will define how AI can be used in defense and intelligence activities such as surveillance and targeting functions. 

On the other side of the Atlantic, the EU leads in AI regulation. Obviously, the AI Act comes to mind, but previous wide-scope legislation applies on these topics as well, notably the Digital Services Act (DSA) and Digital Markets Act (DMA). The [AI Act](https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence), passed in 2024 and fully effective by June 2026, classifies AI services by risk levels: minimal, limited, high, and unacceptable, with stricter rules for high-risk AI systems and prohibitions on unacceptable ones. The [DSA](https://digital-strategy.ec.europa.eu/en/policies/digital-services-act) enforces platform accountability, transparency, content moderation, and user protections for online services such as social media, online travel platforms, and marketplaces. The [DMA](https://digital-markets-act.ec.europa.eu/index_en%20) targets "gatekeeper" platforms that provide core platform services, such as search engines, app stores, and messenger services, to foster fairer, more competitive digital markets.

Big Tech firms are increasingly facing pressure from the EU to comply with AI regulations in the form of heavy fines and investigations. On December 4, the EU [launched](https://www.reuters.com/sustainability/boards-policy-regulation/eu-launch-antitrust-probe-into-meta-over-use-ai-whatsapp-ft-reports-2025-12-04/) an antitrust investigation into Meta Platforms over its policy of rolling out AI features in WhatsApp. The European Commission announced that it will investigate Meta’s new policy as it would restrict rival AI providers and potentially promote its own Meta AI system that was integrated into the platform earlier this year, thus violating competition laws. The EU began another antitrust investigation, this time into Google, on December 9, [examining](https://www.cnbc.com/2025/12/09/google-hit-with-eu-antitrust-probe-over-use-of-online-content-for-ai.html%20) whether Google violated EU competition rules by using web publishers’ content for AI-related purposes. It will also assess to what extent Google’s AI Overviews and AI Mode rely on publishers’ material without providing suitable compensation.

Given this ongoing pressure, Meta has [agreed](https://www.aa.com.tr/en/europe/meta-agrees-to-offer-less-personalized-ad-option-for-eu-users/3765367) to offer EU residents a less-personalised advertising option starting January 2026, to comply with the Digital Markets Act (DMA). After several months of negotiations with the European Commission, Meta will allow users to access its social media platforms without consenting to extensive data processing for fully personalised ads. The Commission had [ruled](https://www.morningstar.com/news/dow-jones/202512083515/meta-to-give-eu-users-choice-on-personalized-ads-to-appease-regulators%20) in April 2025 that Meta had breached the DMA and issued a non-compliance decision over insufficient user choice, and fined the company 200 million euros. Meta [appealed](https://www.reuters.com/sustainability/boards-policy-regulation/meta-may-face-daily-fines-over-pay-or-consent-model-eu-warns-2025-06-27/) the fine calling it unlawful and discriminatory.

Amid intensifying geopolitical and technological rivalry, the EU is attempting to balance competing priorities: strategic autonomy in AI and global standard‑setting influence. While some tech industry observers and policy analysts have [criticised](https://carnegieendowment.org/research/2025/05/the-eus-ai-power-play-between-deregulation-and-innovation?lang=en) the EU for its stringent rules that may deter the investment and expertise required to build a strong AI ecosystem, it has also received [criticism](https://www.aljazeera.com/economy/2025/11/20/eu-moves-to-ease-ai-privacy-rules-amid-pressure-from-big-tech-trump) from digital rights advocates and legal experts for lowering the regulatory standards, thereby prioritising short-term growth of homegrown AI infrastructure over promoting trustworthy and human-centric AI systems. The developments indicative of scaling back oversight include shelving the [Liability Directive](https://commission.europa.eu/business-economy-euro/doing-business-eu/contract-rules/digital-contracts/liability-rules-artificial-intelligence_en), introduction of the [Digital Omnibus](https://www.techpolicy.press/the-eus-digital-omnibus-must-be-rejected-by-lawmakers-here-is-why/), delay in the enforcement of the Digital Services Act (DSA), and introduction of a new set of AI codes of practice that lack firm restraints. The Digital Omnibus legislation was [passed](https://www.3eco.com/article/news-and-insights/eu-parliament-omnibus-passed-far-right-support/) by the EU parliament through an alliance between the European People’s Party (EPP) and far-right parties, many of which are supported by the [US government](https://www.theguardian.com/us-news/2025/dec/05/civilisational-erasure-us-strategy-document-appears-to-echo-far-right-conspiracy-theories-about-europe) and US tech actors such as [Elon Musk](https://www.csmonitor.com/World/Europe/2025/0130/elon-musk-far-right-europe-twitter-x). The legislation includes [weakening](https://www.politico.eu/article/ursula-von-der-leyen-eu-parliament-draft-digital-reforms/#:~:text=On%20the%20issue%20of%20privacy%2C%20though%2C%20some,over%20industry's%20interests%20in%20some%20legislative%20fights.) of oversight mechanisms for AI systems, which will benefit US tech companies. 

The cross-jurisdiction monitoring of platform policies by Open Terms Archive played a key role in identifying how platforms roll out their terms differently across jurisdictions, which can then be understood based on differences in regulation and severity enforcement. The varying levels of notification of data collection purposes for different jurisdictions by LinkedIn or the option of consent and more data control for EU users by Meta are clear examples of how regulations and enforcement impacts big tech policies. The regulatory positions of the US and the EU and the narratives around it also bring to light the framing of regulations as an obstacle for innovation or economic growth, presenting a false dichotomy. Regulations here are presented as a trade-off to AI leadership rather than as an essential means for protecting people and smaller players through digital rights and competition laws.

[^1]: These are US-based organisations with the exception of 5Rights that is headquartered in the UK, but also operates in the US.
